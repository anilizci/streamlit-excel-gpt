# chunked_embeddings.py

import openai
import numpy as np

def split_text(text, chunk_size=200, overlap=50):
    """
    Splits 'text' into chunks of roughly 'chunk_size' words,
    with 'overlap' words carried over between consecutive chunks.
    """
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = ' '.join(words[start:end])
        chunks.append(chunk)
        # Move start forward by chunk_size - overlap
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

def get_embedding(text, model="text-embedding-ada-002"):
    """
    Returns the embedding vector for the given text using OpenAI's Embedding API.
    """
    response = openai.Embedding.create(input=[text], model=model)
    embedding = response['data'][0]['embedding']
    return embedding

def cosine_similarity(vec1, vec2):
    """
    Computes the cosine similarity between two vectors.
    """
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def create_embeddings_for_chunks(chunks):
    """
    For each text chunk, compute its embedding and store both in a list.
    Returns a list of dicts: [{ "chunk": chunk_text, "embedding": [...] }, ...]
    """
    embeddings = []
    for chunk in chunks:
        emb = get_embedding(chunk)
        embeddings.append({
            "chunk": chunk,
            "embedding": emb
        })
    return embeddings

def find_most_similar_chunk(query, embeddings):
    """
    Computes the embedding of the query, compares it with each chunk's embedding,
    and returns the chunk with the highest similarity.
    """
    query_embedding = get_embedding(query)
    max_similarity = -1
    best_chunk = None
    for item in embeddings:
        similarity = cosine_similarity(query_embedding, item["embedding"])
        if similarity > max_similarity:
            max_similarity = similarity
            best_chunk = item["chunk"]
    return best_chunk

def ask_gpt(query, chunk):
    """
    Calls GPT (e.g. GPT-3.5) using a prompt that instructs it to answer
    ONLY with the provided chunk.
    """
    if not chunk:
        return "I don't have information on that."

    prompt = (
        f"Using only the following chunk of knowledge, answer the question below.\n\n"
        f"Knowledge Chunk:\n{chunk}\n\n"
        f"Question: {query}\n\nAnswer:"
    )

    # Example system message for disclaimers
    system_msg = (
        "**Disclaimer: This response was generated by an AI to assist with your inquiry. "
        "While we strive for accuracy, the information provided may not be completely precise. "
        "For more detailed assistance, please contact the SidleyBI Helpdesk.**"
    )

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  # or "gpt-4" if you have access
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        max_tokens=300
    )
    answer = response['choices'][0]['message']['content']
    return answer
